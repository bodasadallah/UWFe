2024-04-20 18:56:10.405962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-20 18:56:10.406092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-20 18:56:10.407180: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-20 18:56:10.413316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-20 18:56:11.588560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package punkt to
[nltk_data]     /home/abdelrahman.sadallah/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [06:22<12:45, 382.76s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [08:22<03:48, 228.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [10:13<00:00, 174.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [10:13<00:00, 204.48s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [01:06<38:50, 66.59s/it]  6%|▌         | 2/36 [02:05<35:15, 62.21s/it]  8%|▊         | 3/36 [03:04<33:24, 60.74s/it] 11%|█         | 4/36 [04:04<32:05, 60.17s/it] 14%|█▍        | 5/36 [05:02<30:47, 59.59s/it] 17%|█▋        | 6/36 [06:01<29:42, 59.42s/it] 19%|█▉        | 7/36 [07:01<28:48, 59.60s/it] 22%|██▏       | 8/36 [08:01<27:54, 59.80s/it] 25%|██▌       | 9/36 [09:03<27:06, 60.25s/it] 28%|██▊       | 10/36 [10:02<25:56, 59.85s/it] 31%|███       | 11/36 [11:01<24:54, 59.77s/it] 33%|███▎      | 12/36 [12:01<23:55, 59.81s/it] 36%|███▌      | 13/36 [13:00<22:48, 59.50s/it] 39%|███▉      | 14/36 [13:59<21:48, 59.47s/it] 42%|████▏     | 15/36 [15:00<20:54, 59.75s/it] 44%|████▍     | 16/36 [15:58<19:45, 59.26s/it] 47%|████▋     | 17/36 [16:57<18:45, 59.25s/it] 50%|█████     | 18/36 [17:57<17:52, 59.59s/it] 53%|█████▎    | 19/36 [18:57<16:51, 59.53s/it] 56%|█████▌    | 20/36 [19:57<15:55, 59.72s/it] 58%|█████▊    | 21/36 [20:55<14:49, 59.33s/it] 61%|██████    | 22/36 [21:54<13:47, 59.11s/it] 64%|██████▍   | 23/36 [22:53<12:49, 59.20s/it] 67%|██████▋   | 24/36 [23:52<11:48, 59.04s/it] 69%|██████▉   | 25/36 [24:51<10:48, 58.97s/it] 72%|███████▏  | 26/36 [25:50<09:49, 58.96s/it] 75%|███████▌  | 27/36 [26:50<08:54, 59.40s/it] 78%|███████▊  | 28/36 [27:48<07:52, 59.04s/it] 81%|████████  | 29/36 [28:48<06:53, 59.14s/it] 83%|████████▎ | 30/36 [29:47<05:54, 59.13s/it] 86%|████████▌ | 31/36 [30:48<04:59, 59.85s/it] 89%|████████▉ | 32/36 [31:48<03:59, 59.75s/it] 92%|█████████▏| 33/36 [32:47<02:58, 59.57s/it] 94%|█████████▍| 34/36 [33:46<01:58, 59.27s/it] 97%|█████████▋| 35/36 [34:44<00:59, 59.06s/it]100%|██████████| 36/36 [35:46<00:00, 59.76s/it]100%|██████████| 36/36 [35:46<00:00, 59.61s/it]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
