2024-04-11 06:32:21.089526: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-11 06:32:21.134005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-11 06:32:21.134088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-11 06:32:21.134994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-11 06:32:21.140440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-11 06:32:22.719249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package punkt to
[nltk_data]     /home/abdelrahman.sadallah/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 28.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.86s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:24<09:37, 144.29s/it] 40%|████      | 2/5 [04:48<07:13, 144.51s/it] 60%|██████    | 3/5 [07:14<04:49, 144.85s/it] 80%|████████  | 4/5 [09:39<02:25, 145.03s/it]100%|██████████| 5/5 [11:52<00:00, 140.79s/it]100%|██████████| 5/5 [11:52<00:00, 142.56s/it]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/home/abdelrahman.sadallah/mbzuai/UWFe/evaluate_llm.py", line 168, in <module>
    with open(save_file, 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/output'
