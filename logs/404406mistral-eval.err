2024-04-20 17:01:05.258526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-20 17:01:05.258993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-20 17:01:05.332413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-20 17:01:05.479338: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-20 17:01:07.083203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package punkt to
[nltk_data]     /home/abdelrahman.sadallah/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [03:23<06:46, 203.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [04:12<01:52, 112.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [05:00<00:00, 82.97s/it] Loading checkpoint shards: 100%|██████████| 3/3 [05:00<00:00, 100.04s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [00:55<32:09, 55.13s/it]  6%|▌         | 2/36 [01:48<30:40, 54.12s/it]  8%|▊         | 3/36 [02:41<29:33, 53.76s/it] 11%|█         | 4/36 [03:35<28:33, 53.56s/it] 14%|█▍        | 5/36 [04:28<27:38, 53.50s/it] 17%|█▋        | 6/36 [05:21<26:41, 53.39s/it] 19%|█▉        | 7/36 [06:14<25:46, 53.33s/it] 22%|██▏       | 8/36 [07:08<24:57, 53.49s/it] 25%|██▌       | 9/36 [08:02<24:03, 53.46s/it] 28%|██▊       | 10/36 [08:55<23:10, 53.48s/it] 31%|███       | 11/36 [09:49<22:16, 53.46s/it] 33%|███▎      | 12/36 [10:42<21:23, 53.49s/it] 36%|███▌      | 13/36 [11:35<20:29, 53.45s/it] 39%|███▉      | 14/36 [12:29<19:37, 53.52s/it] 42%|████▏     | 15/36 [13:23<18:43, 53.50s/it] 44%|████▍     | 16/36 [14:16<17:51, 53.58s/it] 47%|████▋     | 17/36 [15:10<16:57, 53.55s/it] 50%|█████     | 18/36 [16:04<16:04, 53.60s/it] 53%|█████▎    | 19/36 [16:57<15:10, 53.57s/it] 56%|█████▌    | 20/36 [17:50<14:16, 53.51s/it] 58%|█████▊    | 21/36 [18:44<13:21, 53.47s/it] 61%|██████    | 22/36 [19:37<12:28, 53.47s/it] 64%|██████▍   | 23/36 [20:31<11:35, 53.47s/it] 67%|██████▋   | 24/36 [21:24<10:42, 53.53s/it] 69%|██████▉   | 25/36 [22:18<09:48, 53.46s/it] 72%|███████▏  | 26/36 [23:12<08:55, 53.57s/it] 75%|███████▌  | 27/36 [24:05<08:02, 53.62s/it] 78%|███████▊  | 28/36 [24:59<07:08, 53.62s/it] 81%|████████  | 29/36 [25:53<06:15, 53.61s/it] 83%|████████▎ | 30/36 [26:46<05:21, 53.58s/it] 86%|████████▌ | 31/36 [27:39<04:27, 53.53s/it] 89%|████████▉ | 32/36 [28:33<03:34, 53.61s/it] 92%|█████████▏| 33/36 [29:27<02:40, 53.60s/it] 94%|█████████▍| 34/36 [30:21<01:47, 53.67s/it] 97%|█████████▋| 35/36 [31:14<00:53, 53.55s/it]100%|██████████| 36/36 [32:09<00:00, 53.88s/it]100%|██████████| 36/36 [32:09<00:00, 53.58s/it]
Traceback (most recent call last):
  File "/home/abdelrahman.sadallah/mbzuai/UWFe/evaluate_llm.py", line 245, in <module>
    assert len(predicitons) == len(outputs) == len(cleaned_predictions)
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/cognitivecomputations/dolphin-2.8-mistral-7b-v02_outputs_0-shots_True-explicit.txt'
