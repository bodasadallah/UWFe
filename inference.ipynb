{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb867be9-709c-4214-b6ee-fdd65e386be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 22:07:15.199034: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-24 22:07:15.199396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-24 22:07:15.200403: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-24 22:07:15.207087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 22:07:16.744404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/abdelrahman.sadallah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers import HfArgumentParser, Seq2SeqTrainingArguments,EarlyStoppingCallback\n",
    "\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional\n",
    "from datasets import load_dataset, concatenate_datasets,Value\n",
    "import numpy as np\n",
    "from typing import Union, Optional\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset, AutoModel\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    #glue_compute_metrics,\n",
    "    glue_output_modes,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,GenerationConfig\n",
    ")\n",
    "from arguments import ModelArguments, DataArguments\n",
    "import wandb\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from evaluate import load\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from evaluate import load\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from peft import PeftModel    \n",
    "import logging\n",
    "import os\n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39875fba-5200-407d-83d1-282b62650b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b5b03f92d04adbb672f2f122174ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FULL_MODEL_NAME=\"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(FULL_MODEL_NAME)\n",
    "val_dataset = get_dataset(\n",
    "    dataset_path = \"boda/kaneko_data\",\n",
    "    split='test',\n",
    "    field='prompt',\n",
    "    num_shots=0,\n",
    "    explicit_errors = 1)\n",
    "\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4,collate_fn=lambda x: x )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0ae736-0d62-45fc-9667-ddba9891d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef3d91e1eba49c2b46b8f076e7819c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            # bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=False\n",
    "        )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        FULL_MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=model_args.use_flash_attention_2,\n",
    "        use_flash_attention_2=0,\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cc8328-5c1f-4d1f-9c22-2b696b8ae1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH= '/l/users/abdelrahman.sadallah/UWFE-mistral-explicit-errors/mistralai/Mistral-7B-v0.1/best'\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(model, CHECKPOINT_PATH)\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99b57abf-ab2d-4c13-8de0-4eed1d27fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompts, tokenizer, model):\n",
    "    \n",
    "   \n",
    "    encoding = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **encoding,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            temperature=0.0001,\n",
    "            repetition_penalty = 5.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            num_beams=10,\n",
    "            # generation_config=generation_config,\n",
    "        )  \n",
    "    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "    output_text = tokenizer.batch_decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93daed19-9798-43f2-affc-00a19281d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                  | 0/36 [01:12<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define PAD Token = BOS Token\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "model.config.pad_token_id = model.config.bos_token_id\n",
    "\n",
    "predicitons = []\n",
    "outputs = []     \n",
    "for batch in tqdm(val_dataloader):\n",
    "\n",
    "    prompts = [x['prompt'] for x in batch]\n",
    "    ans = []\n",
    "    \n",
    "    output_text = inference(prompts=prompts, tokenizer=tokenizer, model=model)\n",
    "    labels = []\n",
    "    for i,ex in enumerate(batch):\n",
    "        # labels.append(f\"{i+1}. {ex['explanation']}\\n\")\n",
    "        labels.append(ex['label'])\n",
    "    \n",
    "    predicitons.extend(output_text)\n",
    "    outputs.extend(labels)\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c140dfe-f63c-4670-b243-b1665be91ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_sentence: Therefore , the lack of creative qualified human resource seriously hampers engineering design processes for innovations in China .\n",
      "\n",
      "Correct_sentence: Therefore , the lack of creative human resources seriously hampers engineering design processes for innovations in China .\n",
      "\n",
      "Prediction: \n",
      "1. The adjective \"creative\" modifies the noun \"human resources,\" so it doesn't need to be repeated.\n",
      "2. This sentence is talking about human resources in general, so \"resource\" should be pluralized. Also, the verb \"hampers\" is not used with countable nouns, so \"resources\" should be changed to the uncountable noun \"resource.\" \n",
      "3. None\n",
      "\n",
      "Label: 1. The adjective \"qualified\" means having the necessary skills to perform a particular task, but it refers mainly to official or certified qualifications, such as tests. There is no test that would certify someone's creative thinking abilities, so \"qualified\" should be omitted. The sentence could also be reworded to \"human resources with creative thinking abilities,\" but \"creative human resources\" expresses the same idea more concisely and with less potential for confusion.\n",
      "2. The phrase \"human resources\" refers to a group of people who are able to do work. Because it refers to a group, it is only used in the plural in principle. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error_sentence: On the other side , as stated above , a person who want to confirm and has been prepared to face the result can choose to do the genetic testing individually .\n",
      "\n",
      "Correct_sentence: On the other hand , as stated above , a person who wants to confirm and has been prepared to face the result can choose to do the genetic testing individually .\n",
      "\n",
      "Prediction: \n",
      "1. The idiom is \"on the other hand,\" not \"on the other side.\" \n",
      "2. Incorrect conjugation. Use the third person singular \"wants\" to go with \"a person.\" Also, you need to add an apostrophe to make it possessive: \"a person who wants to confirm...\" or \"a person who wants to confirm...\" instead of \"a person who want to confirm...\" (with no apostrophe) or \"a person that wants to confirm...\" (incorrect use of \"that\") or \"a person which wants to confirm...\" (incorrect use of \"which\").\n",
      "\n",
      "Label: 1. The phrase \"on the other side\" is likely understandable, but is not really used in English. The sentence is much easier to understand with the common set phrase idiom \"on the other hand.\"\n",
      "2. The subject, \"a person\" is singular, and so for the sake of subject-verb agreement, the verb should be \"wants,\" not \"want.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error_sentence: One of the disadvantage that many people concerns is that people loss the interest of face-to-face communication .\n",
      "\n",
      "Correct_sentence: One of the disadvantages that many people concerns is that people loss interest in face-to-face communication .\n",
      "\n",
      "Prediction: \n",
      "1. This sentence is talking about one of multiple disadvantages, so \"disadvantage\" should be pluralised.\n",
      "2. The concept of \"interest\" is a general concept and can be assumed to be understood by the reader. It takes the definite article only when referring to a specific interest, instead of interests in general.\n",
      "3. Incorrect preposition. Use \"in\" to indicate what something is interested in. \n",
      "4. None\n",
      "\n",
      "Label: 1. The noun \"disadvantage\" requires either pluralization or an article. It refers to a result of something, so it is often plural because it is assumed that any action can cause multiple results. Unless it is known that there is only one singular disadvantage, usually it is used in the plural. Also, the phrase \"one of\" implies that there are multiple potential members of a group, so what comes after it must be plural.\n",
      "2. I partially agree with the correction because \"the\" should be deleted, but that does not fix the issue. The phrase \"lose interest in\" is a commonly used expression, and it never uses an article. The first word is always a verb form, and \"loss\" is a noun, so it should be changed to \"lose.\"\n",
      "3. The phrase \"lose interest in\" is a commonly used expression, and it always uses the article \"in.\" The phrase \"lose interest of\" is incorrect, and it would mean that the \"interest\" belongs to face-to-face communication or is an aspect of it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error_sentence: I agree that RFID technology shall not be made available to the public for easy abuse and distorted usage .\n",
      "\n",
      "Correct_sentence: I agree that RFID technology should not be made available to the public for easy abuse and distorted usage .\n",
      "\n",
      "Prediction: \n",
      "1. \"Shall\" is an archaic form of the modal verb \"will,\" which is used to express certainty about the future. The correct expression is \"I agree that RFID technology will not be made available to the public for easy abuse and distorted usage.\" Also, \"should\" is more commonly used than \"shall\" in modern English. Therefore, the original sentence should be changed from \"shall\" to \"should.\"\n",
      "\n",
      "Label: 1. \"Shall\" is an old-fashioned way of expressing future tense and is not commonly used in modern English. \"Should\" is a more commonly used word to express an opinion. The author believes making RFID available to the public is not beneficial, so \"should not\" is used. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predicitons)):\n",
    "            print(f\"Error_sentence: {val_dataset[i]['incorrect_sentence']}\\n\")\n",
    "            print(f\"Correct_sentence: {val_dataset[i]['correct_sentence']}\\n\")\n",
    "            print(f\"Prediction: {predicitons[i]}\\n\")\n",
    "            print(f\"Label: {outputs[i]}\\n\")\n",
    "            print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071f158d-7faa-4225-9654-508695781cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 12:02:21.254619: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-25 12:02:21.254877: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-25 12:02:21.302062: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-25 12:02:21.402663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 12:02:23.045192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from utils import get_dataset\n",
    "\n",
    "ds =  get_dataset(\n",
    "    dataset_path = \"boda/kaneko_data\",\n",
    "    split='test',\n",
    "    field='prompt',\n",
    "    num_shots=5,\n",
    "    explicit_errors = 0).select([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe6738e-b23e-4c37-8320-7f3180f0a953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['corrections', 'incorrect_sentence', 'correct_sentence', 'prompt', 'label'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
