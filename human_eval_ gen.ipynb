{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca50a0d0-38eb-49ed-b08c-5999a2f57b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 17:50:58.649229: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 17:50:58.649632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 17:50:58.715261: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-26 17:50:58.861551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 17:51:00.452041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/abdelrahman.sadallah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers import HfArgumentParser, Seq2SeqTrainingArguments,EarlyStoppingCallback\n",
    "\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional\n",
    "from datasets import load_dataset, concatenate_datasets,Value\n",
    "import numpy as np\n",
    "from typing import Union, Optional\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset, AutoModel\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    #glue_compute_metrics,\n",
    "    glue_output_modes,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,GenerationConfig\n",
    ")\n",
    "from arguments import ModelArguments, DataArguments\n",
    "import wandb\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from evaluate import load\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from evaluate import load\n",
    "# from utils import get_dataset\n",
    "import numpy as np\n",
    "from peft import PeftModel    \n",
    "import logging\n",
    "import os\n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921dde27-51df-4d0e-a721-5d459e313d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bisect import bisect_left\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from evaluate import load\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "## Few-shots + explicit_errors\n",
    "few_shots_explicit = '''You are given a few examples of erroneous sentences and their corrections. For every example, you are given the explicit errors and their corrections. You are also given an erroneous input sentence and its correction, followed by the explicit errors in this sentence. Output a simple explanation for each error in the erroneous sentence.'''\n",
    "\n",
    "## Few shots + no explicit errors\n",
    "few_shots_no_explicit = '''You are given a few examples of erroneous sentences and their corrections. You are also given an erroneous input sentence and its correction. Output a simple explanation for each error in the erroneous sentence.'''\n",
    "\n",
    "## No few-shots + explicit\n",
    "no_few_shots_explicit = '''You are given an erroneous input sentence and its correction, followed by the explicit errors in this sentence. Output a simple explanation for each error in the erroneous sentence.'''\n",
    "\n",
    "## No few-shots + no explicit\n",
    "no_few_shots_no_explicit = '''You are given an erroneous input sentence and its correction. Output a simple explanation for each error in the erroneous sentence.'''\n",
    "\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "def put_explicit_error(example):\n",
    "    ret = ''\n",
    "    ret += f\"Errors:\\n\"\n",
    "\n",
    "    for i,correction in enumerate(example['corrections']):\n",
    "        correct_word = ' ' if correction['correct'] == '-' else correction['correct']\n",
    "        error_word = ' ' if correction['error'] == '-' else correction['error']\n",
    "\n",
    "        ret += f\"{i+1}. Error: {error_word}, Correction: {correct_word}\\n\"\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "def generate_prompt(example, prompt_head = None,num_shots=0, explicit_errors = 0,train = 1, dataset = None, field = None):\n",
    "    full_prompt  = prompt_head\n",
    "\n",
    "    # print(example.keys())\n",
    "\n",
    "    if num_shots:\n",
    "        idx= np.random.randint(0,len(dataset),num_shots)\n",
    "        samples = dataset.select(idx)\n",
    "\n",
    "        for sample in samples:\n",
    "            full_prompt += f\"Erroneous sentence: {sample['incorrect_sentence']}\\n\"\n",
    "            full_prompt += f\"Correct sentence: {sample['correct_sentence']}\\n\"\n",
    "            if explicit_errors:\n",
    "                full_prompt += put_explicit_error(sample)\n",
    "\n",
    "            full_prompt += f\"Explanations:\\n\"\n",
    "            for i,correction in enumerate(sample['corrections']):\n",
    "                full_prompt += f\"{i+1}. {correction['explanation']}\\n\"\n",
    "            \n",
    "    full_prompt += f\"Erroneous sentence: {example['incorrect_sentence']}\\n\"\n",
    "    full_prompt += f\"Correct sentence: {example['correct_sentence']}\\n\"\n",
    "\n",
    "    if explicit_errors:\n",
    "        full_prompt += put_explicit_error(example)\n",
    "        \n",
    "    full_prompt += f\"Explanations:\\n\"\n",
    "\n",
    "    labels = ''\n",
    "    for i,correction in enumerate(example['corrections']):\n",
    "        labels += f\"{i+1}. {correction['explanation']}\\n\"\n",
    "    if train:\n",
    "            full_prompt += labels\n",
    "\n",
    "\n",
    "    # example['text'] = 'sd'\n",
    "    example[field] = full_prompt.strip()\n",
    "    example['label'] = labels\n",
    "    # print(example.keys())\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset_path, split='train', field='prompt', num_shots=0,explicit_errors = 0):\n",
    "    \n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_path)[split]\n",
    "\n",
    "    ########### Choose the prompt head based on the options ###########\n",
    "    if num_shots and explicit_errors:\n",
    "        prompt_head = few_shots_explicit\n",
    "    elif num_shots and not explicit_errors:\n",
    "        prompt_head = few_shots_no_explicit\n",
    "    elif not num_shots and explicit_errors:\n",
    "        prompt_head = no_few_shots_explicit\n",
    "    else:\n",
    "        prompt_head = no_few_shots_no_explicit\n",
    "\n",
    "    dataset = dataset.map(generate_prompt, fn_kwargs={\"field\": field, \\\n",
    "        \"prompt_head\": prompt_head, \"train\": split == 'train', \\\n",
    "        'num_shots': num_shots,'dataset':dataset , 'explicit_errors': explicit_errors})\n",
    "        \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63cdf678-bb69-4b3c-8019-05dcc89e1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs = [\n",
    "#     { 'model_name': 'cognitivecomputations/dolphin-2.8-mistral-7b-v02',\n",
    "#      'chkpt': \"/l/users/abdelrahman.sadallah/UWFE-Mixtral/cognitivecomputations/dolphin-2.8-mistral-7b-v02/best\",\n",
    "#      'shots': 0,\n",
    "#     'explicit_errors': 0\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mistral-7B-v0.1',\n",
    "#      'chkpt': \"/l/users/abdelrahman.sadallah/UWFE-mistral-explicit-errors/mistralai/Mistral-7B-v0.1/best\",\n",
    "#      'shots': 0,\n",
    "#     'explicit_errors': 1\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mistral-7B-v0.1',\n",
    "#      'chkpt': \"/l/users/abdelrahman.sadallah/UWFE-Mixtral/mistralai/Mistral-7B-v0.1/best/\",\n",
    "#      'shots': 0,\n",
    "#     'explicit_errors': 0\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mixtral-8x7B-v0.1',\n",
    "#      'chkpt': \"/l/users/abdelrahman.sadallah/UWFE-mixtral-explicit-errors/mistralai/Mixtral-8x7B-v0.1/checkpoint-18500/\",\n",
    "#      'shots': 0,\n",
    "#     'explicit_errors': 1\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mistral-7B-v0.1',\n",
    "#         'chkpt': \"\",\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 0,\n",
    "#         'base': True\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mistral-7B-v0.1',\n",
    "#         'chkpt': \"\",\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 1,\n",
    "#         'base': True\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mistral-7B-v0.1',\n",
    "#         'chkpt': \"\",\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 0,\n",
    "#         'base': True\n",
    "#     },\n",
    "#     { 'model_name': 'mistralai/Mistral-7B-v0.1',\n",
    "#         'chkpt': \"\",\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 1,\n",
    "#         'base': True\n",
    "#     },\n",
    "#     { 'model_name': 'Meta-Llama-3-8B',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B',\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 0,\n",
    "#     },\n",
    "#     { 'model_name': 'Meta-Llama-3-8B',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B',\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 1,\n",
    "#     },\n",
    "#     { 'model_name': 'Meta-Llama-3-8B',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B',\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 0,\n",
    "#     },\n",
    "\n",
    "#     { 'model_name': 'Meta-Llama-3-8B',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B',\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 1,\n",
    "#     },\n",
    "#     { 'model_name': 'Meta-Llama-3-8B-Instruct',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B-Instruct',\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 0,\n",
    "#     },\n",
    "#     { 'model_name': 'Meta-Llama-3-8B-Instruct',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B-Instruct',\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 1,\n",
    "#     },\n",
    "#     { 'model_name': 'Meta-Llama-3-8B-Instruct',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B-Instruct',\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 0,\n",
    "#     },\n",
    "\n",
    "#     { 'model_name': 'Meta-Llama-3-8B-Instruct',\n",
    "#         'chkpt': '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B-Instruct',\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 1,\n",
    "#     },\n",
    "#     { 'model_name': 'chatgpt',\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 0,\n",
    "#      },\n",
    "#     { 'model_name': 'chatgpt',\n",
    "#         'shots': 0,\n",
    "#         'explicit_errors': 1,\n",
    "#      },\n",
    "#     { 'model_name': 'chatgpt',\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 0,\n",
    "#      },\n",
    "#     { 'model_name': 'chatgpt',\n",
    "#         'shots': 5,\n",
    "#         'explicit_errors': 1,\n",
    "#      }\n",
    "\n",
    "\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "# import random\n",
    "\n",
    "# ids = random.sample(range(0, 100), 20)\n",
    "# for i,c in enumerate(configs):\n",
    "#     configs[i]['id'] = str(ids[i])\n",
    "\n",
    "# random.shuffle(configs)\n",
    "\n",
    "# import json\n",
    "# with open('evaluation_sheet.json','w') as f:\n",
    "#      json.dump(configs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f53a252-b0d6-471e-a7e3-61ab76ec9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shot_no_explicit_val_dataset = get_dataset(\n",
    "    dataset_path = \"boda/kaneko_data\",\n",
    "    split='test',\n",
    "    field='prompt',\n",
    "    num_shots=0,\n",
    "    explicit_errors = 0).select([0,7,77,54,15,48,55,100,97,31])\n",
    "\n",
    "no_shot_explicit_val_dataset = get_dataset(\n",
    "    dataset_path = \"boda/kaneko_data\",\n",
    "    split='test',\n",
    "    field='prompt',\n",
    "    num_shots=0,\n",
    "    explicit_errors = 1).select([0,7,77,54,15,48,55,100,97,31])\n",
    "\n",
    "five_shot_no_explicit_val_dataset = get_dataset(\n",
    "    dataset_path = \"boda/kaneko_data\",\n",
    "    split='test',\n",
    "    field='prompt',\n",
    "    num_shots=5,\n",
    "    explicit_errors = 0).select([0,7,77,54,15,48,55,100,97,31])\n",
    "\n",
    "five_shot_explicit_val_dataset = get_dataset(\n",
    "    dataset_path = \"boda/kaneko_data\",\n",
    "    split='test',\n",
    "    field='prompt',\n",
    "    num_shots=5,\n",
    "    explicit_errors = 1).select([0,7,77,54,15,48,55,100,97,31])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c7237c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "configs = []\n",
    "with open('evaluation_sheet.json') as json_file:\n",
    "   configs = json.load(json_file)\n",
    "\n",
    "from os import walk\n",
    "\n",
    "done = []\n",
    "for (dirpath, dirnames, filenames) in walk('human_eval'):\n",
    "    for fn in filenames:\n",
    "        done.append(fn.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98fe2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(filename,predicitons, labels, Errors, corrects ):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(predicitons)):\n",
    "            f.write(f\"Error_sentence: {Errors[i]}\\n\\n\")\n",
    "            f.write(f\"Correct_sentence: {corrects[i]}\\n\\n\")\n",
    "            f.write(f\"Prediction: {predicitons[i]}\\n\\n\")\n",
    "            f.write(f\"Label: {labels[i]}\\n\\n\")\n",
    "            f.write(f'-' * 30)\n",
    "            f.write('\\n\\n\\n')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc72364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "from tqdm import tqdm   \n",
    "\n",
    "def chatgpt_inf(val_dataset,c):\n",
    "\n",
    "    id = c['id']\n",
    "    model_name = \"gpt-3.5-turbo-0125\"\t\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    error_sentences = []\n",
    "    correct_sentences = []\n",
    "    for example in tqdm(val_dataset):\n",
    "        \n",
    "        prompt = example['prompt']\n",
    "        response = None\n",
    "        try:\n",
    "            clue_message = {\"role\": \"user\", \"content\":prompt}\n",
    "            completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                clue_message])\n",
    "            response = completion.choices[0].message.content.lower()\n",
    "            outputs.append(response)\n",
    "            labels.append(example[\"label\"])\n",
    "            error_sentences.append(example[\"incorrect_sentence\"])\n",
    "            correct_sentences.append(example[\"correct_sentence\"])\n",
    "\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "    \n",
    "    filename = f\"human_eval/{id}.txt\"\n",
    "    assert len(outputs) == len(labels) == len(error_sentences) == len(correct_sentences)\n",
    "\n",
    "    write_output(filename,outputs, labels, error_sentences, correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6eb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_inf(val_dataloader,c):\n",
    "\n",
    "    import os\n",
    "    os.environ['RANK'] = '0'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    if c['model_name'] == 'Meta-Llama-3-8B-Instruct':\n",
    "        tokenizer_path = '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B-Instruct/tokenizer.model'\n",
    "    elif c['model_name'] == 'Meta-Llama-3-8B':\n",
    "        tokenizer_path = '/l/users/abdelrahman.sadallah/llama3/Meta-Llama-3-8B/tokenizer.model'\n",
    "\n",
    "    generator = Llama.build(\n",
    "        ckpt_dir=c['chkpt'],\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_seq_len=2048,\n",
    "        max_batch_size=2,\n",
    "        model_parallel_size = 1\n",
    "        )\n",
    "    \n",
    "    predicitons = []\n",
    "    labels = []\n",
    "    error_sentences = []\n",
    "    correct_sentences = []\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        prompts = [x['prompt'] for x in batch]\n",
    "        for i,ex in enumerate(batch):\n",
    "            # labels.append(f\"{i+1}. {ex['explanation']}\\n\")\n",
    "            labels.append(ex['label'])\n",
    "            error_sentences.append(ex['incorrect_sentence'])\n",
    "            correct_sentences.append(ex['correct_sentence'])\n",
    "\n",
    "        if c['model_name'] == 'Meta-Llama-3-8B-Instruct':\n",
    "                dialogs: List[Dialog] = [ ]\n",
    "                for p in prompts:\n",
    "                    d = [{\"role\": \"user\", \"content\":p}]\n",
    "                    dialogs.append(d)\n",
    "\n",
    "                prompts = dialogs\n",
    "\n",
    "                model_outputs = generator.chat_completion(\n",
    "                        prompts,\n",
    "                        max_gen_len=256,\n",
    "                        temperature=0.0001,\n",
    "                        # top_p=0.9,\n",
    "                        )\n",
    "        else:\n",
    "            model_outputs = generator.text_completion(\n",
    "                    prompts,\n",
    "                    max_gen_len=256,\n",
    "                    temperature=0.0001,\n",
    "                    # top_p=0.9,\n",
    "                    )\n",
    "\n",
    "        predicitons.extend([z['generation'] for z in model_outputs] if c['model_name'] == 'Meta-Llama-3-8B' else [z['generation']['content'] for z in model_outputs])\n",
    "\n",
    "    filename = f\"human_eval/{c['id']}.txt\"\n",
    "    assert len(predicitons) == len(labels) == len(error_sentences) == len(correct_sentences)\n",
    "    write_output(filename,predicitons, labels, error_sentences, correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d74090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(prompts, tokenizer, model):\n",
    "    \n",
    "   \n",
    "    encoding = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **encoding,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            repetition_penalty = 5.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            num_beams=3,\n",
    "            # generation_config=generation_config,\n",
    "        )\n",
    "    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "    output_text = tokenizer.batch_decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return output_text\n",
    "        \n",
    "\n",
    "def inf(val_dataloader,c):\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            # bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=False\n",
    "        )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    c['model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    # use_flash_attention_2=model_args.use_flash_attention_2,\n",
    "    use_flash_attention_2=0,\n",
    "\n",
    "    )\n",
    "\n",
    "    if c['chkpt']:\n",
    "        print(f\"Loading model from {c['model_name']}\")\n",
    "        adapter_checkpoint  = c['chkpt']\n",
    "        model = PeftModel.from_pretrained(model, adapter_checkpoint)\n",
    "\n",
    "    else:\n",
    "        print(f\"Loading Base Model {c['model_name']}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(c['model_name'])\n",
    "    model = model.eval()\n",
    "    # Define PAD Token = BOS Token\n",
    "    tokenizer.pad_token = tokenizer.bos_token\n",
    "    model.config.pad_token_id = model.config.bos_token_id\n",
    "    predicitons = []\n",
    "    labels = []\n",
    "    error_sentences = []\n",
    "    correct_sentences = []\n",
    "    \n",
    "    for batch in tqdm(val_dataloader):\n",
    "\n",
    "        prompts = [x['prompt'] for x in batch]\n",
    "        for i,ex in enumerate(batch):\n",
    "            # labels.append(f\"{i+1}. {ex['explanation']}\\n\")\n",
    "            labels.append(ex['label'])\n",
    "            error_sentences.append(ex['incorrect_sentence'])\n",
    "            correct_sentences.append(ex['correct_sentence'])\n",
    "\n",
    "        output_text = inference(prompts=prompts, tokenizer=tokenizer, model=model)\n",
    "\n",
    "        predicitons.extend(output_text)\n",
    "\n",
    "    filename = f\"human_eval/{c['id']}.txt\"\n",
    "\n",
    "    \n",
    "    assert len(predicitons) == len(labels) == len(error_sentences) == len(correct_sentences)\n",
    "    write_output(filename,predicitons, labels, error_sentences, correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7f6f9c-ed2d-4b0f-ae21-f319c0d97988",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3 move Mixtral to last\n",
    "for c in configs:\n",
    "    if c['model_name'] == \"mistralai/Mixtral-8x7B-v0.1\":\n",
    "        configs.remove(c)\n",
    "        configs.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a286e3-5d05-427c-b961-aacd204f7f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cognitivecomputations/dolphin-2.8-mistral-7b-v02\n",
      "Meta-Llama-3-8B\n",
      "mistralai/Mistral-7B-v0.1\n",
      "Meta-Llama-3-8B\n",
      "chatgpt\n",
      "chatgpt\n",
      "mistralai/Mistral-7B-v0.1\n",
      "Meta-Llama-3-8B-Instruct\n",
      "Meta-Llama-3-8B-Instruct\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.1\n",
      "Meta-Llama-3-8B-Instruct\n",
      "chatgpt\n",
      "mistralai/Mistral-7B-v0.1\n",
      "Meta-Llama-3-8B-Instruct\n",
      "chatgpt\n",
      "Meta-Llama-3-8B\n",
      "Meta-Llama-3-8B\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "for c in configs:\n",
    "    print(c['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "541b0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating for mistralai/Mistral-7B-v0.1 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf24b78335748d98e156a19a5e2b811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:21<00:00, 28.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating for mistralai/Mistral-7B-v0.1 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333258ee97884620866f923ee6d6be8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:14<00:00, 14.97s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for c in configs:\n",
    "\n",
    "    if str(c['id']) in done:\n",
    "        continue\n",
    "\n",
    "    # if c['model_name'] == 'cognitivecomputations/dolphin-2.8-mistral-7b-v02':\n",
    "    #     continue\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'----- Generating for {c[\"model_name\"]} -----')\n",
    "    \n",
    "    if c['shots'] == 0 and c['explicit_errors'] == 0:\n",
    "        val_dataset = no_shot_no_explicit_val_dataset\n",
    "    elif c['shots'] == 0 and c['explicit_errors'] == 1:\n",
    "        val_dataset = no_shot_explicit_val_dataset\n",
    "    elif c['shots'] == 5 and c['explicit_errors'] == 0:\n",
    "        val_dataset = five_shot_no_explicit_val_dataset\n",
    "    elif c['shots'] == 5 and c['explicit_errors'] == 1:\n",
    "        val_dataset = five_shot_explicit_val_dataset\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2,collate_fn=lambda x: x )\n",
    "\n",
    "    ## chatgpt inference\n",
    "    if c['model_name'] == 'chatgpt':\n",
    "        chatgpt_inf(val_dataset,c)\n",
    "    elif 'Meta' in c['model_name']:\n",
    "        llama_inf(val_dataloader,c)\n",
    "    ## Mistral and mixtral inference\n",
    "    else:\n",
    "        inf(val_dataloader,c)\n",
    "\n",
    "    done.append(str(c['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2edbdfb-6e3e-4811-9050-29710817a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in configs:\n",
    "#  ## chatgpt inference\n",
    "#     if c['model_name'] == 'chatgpt':\n",
    "#         chatgpt_inf(val_dataset,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97db5868-0e76-460b-9349-fcdff3a307c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.environ['WORLD_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09911072-e526-4b66-97ff-72c9b13b880e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '42',\n",
       " '68',\n",
       " '53',\n",
       " '3',\n",
       " '69',\n",
       " '89',\n",
       " '58',\n",
       " '24',\n",
       " '87',\n",
       " '18',\n",
       " '11',\n",
       " '72',\n",
       " '62',\n",
       " '90',\n",
       " '74',\n",
       " '8',\n",
       " '7',\n",
       " '76',\n",
       " '92']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "387adacd-6a6a-491a-adab-6a92c9a056cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given an erroneous input sentence and its correction, followed by the explicit errors in this sentence. Output a simple explanation for each error in the erroneous sentence.Erroneous sentence: Nevertheless , there are cyber polices who roam around these social networking platforms to prevent these hoaxes spreading around .\\nCorrect sentence: Nevertheless , there are cyber police who roam around these social networking platforms to prevent these hoaxes spreading around .\\nErrors:\\n1. Error: polices, Correction: police\\nExplanations:'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_shot_explicit_val_dataset[1]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f5b6e-f14e-4d57-9868-c6fd0c40fed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
